\PassOptionsToPackage{usenames}{color}
\documentclass[11pt]{article}
\usepackage{comment}
\usepackage{relsize} % relative font sizes (e.g. \smaller). must precede ACL style
\usepackage{acl2014}

\usepackage[linkcolor=blue]{hyperref}

\usepackage[round]{natbib}
\newcommand{\citeposs}[1]{\citeauthor{#1}'s (\citeyear{#1})}

%\usepackage{times}
%\usepackage{latexsym}


\usepackage[boxed]{algorithm2e}
\renewcommand\AlCapFnt{\small}
\usepackage[small,bf,skip=5pt]{caption}
\usepackage{sidecap} % side captions
\usepackage{rotating}	% sideways

% Italicize subparagraph headings
\usepackage{titlesec}
\titleformat*{\subparagraph}{\itshape}
\titlespacing{\subparagraph}{%
  1em}{%              left margin
  0pt}{% space before (vertical)
  1em}%               space after (horizontal)

% Numbered Examples and lists
\usepackage{lingmacros}

\usepackage{enumitem} % customizable lists
\setitemize{noitemsep,topsep=0em,leftmargin=*}
\setenumerate{noitemsep,leftmargin=0em,itemindent=13pt,topsep=0em}


\usepackage{textcomp}
% \usepackage{arabtex} % must go after xparse, if xparse is used!
%\usepackage{utf8}
% \setcode{utf8} % use UTF-8 Arabic
% \newcommand{\Ar}[1]{\RL{\novocalize #1}} % Arabic text

\usepackage[procnames]{listings}

\usepackage{amssymb}	%amsfonts,eucal,amsbsy,amsthm,amsopn
\usepackage{amsmath}

\usepackage{mathptmx}	% txfonts
\usepackage[scaled]{beramono}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}

\usepackage{MnSymbol}	% must be after mathptmx

\usepackage{latexsym}

\usepackage{microtype}



% Tables
\usepackage{array}
\usepackage{multirow}
\usepackage{booktabs} % pretty tables
\usepackage{multicol}
\usepackage{footnote}


\usepackage{url}
\usepackage[usenames]{color}
\usepackage{xcolor}

% colored frame box
\newcommand{\cfbox}[2]{%
    \colorlet{currentcolor}{.}%
    {\color{#1}%
    \fbox{\color{currentcolor}#2}}%
}

\usepackage[normalem]{ulem} % \uline
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{subcaption}
%\usepackage{tikz-dependency}
\usepackage{tikz}
%\usepackage{tree-dvips}
\usetikzlibrary{arrows,positioning,calc} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{verbatim}
\usepackage{ulem}


% Author comments
\usepackage{color}
\newcommand\bmmax{0} % magic to avoid 'too many math alphabets' error
\usepackage{bm}
\definecolor{orange}{rgb}{1,0.5,0}
\definecolor{mdgreen}{rgb}{0,0.6,0}
\definecolor{mdblue}{rgb}{0,0,0.7}
\definecolor{dkblue}{rgb}{0,0,0.5}
\definecolor{dkgray}{rgb}{0.3,0.3,0.3}
\definecolor{slate}{rgb}{0.25,0.25,0.4}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{ltgray}{rgb}{0.7,0.7,0.7}
\definecolor{purple}{rgb}{0.7,0,1.0}
\definecolor{lavender}{rgb}{0.65,0.55,1.0}

% Settings for algorithm listings
\makeatletter
\lst@AddToHook{EveryPar}{%
  \label{lst:\thelstnumber}% make a label for each line number except the first (assumes only one listing in the document)
}
\makeatother
\lstset{
% basicstyle=\rmshape,
  numbers=left,
  numberstyle=\tt\color{gray},
  firstnumber=2,
  stepnumber=5,
  xleftmargin=3em,
  language=Python,
  upquote=true,
  showstringspaces=false,
  formfeed=\newpage,
  tabsize=1,
  stringstyle=\color{mdgreen},
  commentstyle=\itshape\color{lavender},
  basicstyle=\small\smaller\ttfamily,
  morekeywords={lambda,with,as,assert},
  keywordstyle=\bfseries\color{magenta},
  procnamekeys={def},
  procnamestyle=\bfseries\color{orange},
  aboveskip=0.5cm,
  belowskip=0.5cm
}
\renewcommand{\lstlistingname}{Algorithm}


\newcommand{\ensuretext}[1]{#1}
\newcommand{\cjdmarker}{\ensuretext{\textcolor{green}{\ensuremath{^{\textsc{CJ}}_{\textsc{D}}}}}}
\newcommand{\nssmarker}{\ensuretext{\textcolor{magenta}{\ensuremath{^{\textsc{NS}}_{\textsc{S}}}}}}
\newcommand{\abmarker}{\ensuretext{\textcolor{red}{\ensuremath{^{\textsc{A}}_{\textsc{B}}}}}}
\newcommand{\ytmarker}{\ensuretext{\textcolor{blue}{\ensuremath{^{\textsc{Y}}_{\textsc{T}}}}}}
\newcommand{\ftamarker}{\ensuretext{\textcolor{orange}{\ensuremath{^{\textsc{FT}}_{\textsc{A}}}}}}
\newcommand{\lslmarker}{\ensuretext{\textcolor{purple}{\ensuremath{^{\textsc{LS}}_{\textsc{L}}}}}}
\newcommand{\arkcomment}[3]{\ensuretext{\textcolor{#3}{[#1 #2]}}}
%\newcommand{\arkcomment}[3]{}
\newcommand{\cjd}[1]{\arkcomment{\cjdmarker}{#1}{green}}
\newcommand{\nss}[1]{\arkcomment{\nssmarker}{#1}{magenta}}
\newcommand{\ab}[1]{\arkcomment{\abmarker}{#1}{red}}
\newcommand{\yt}[1]{\arkcomment{\ytmarker}{#1}{blue}}
\newcommand{\fta}[1]{\arkcomment{\ftamarker}{#1}{orange}}
\newcommand{\lsl}[1]{\arkcomment{\lslmarker}{#1}{purple}}
\newcommand{\params}{\mathbf{\theta}}
\newcommand{\wts}{\mathbf{w}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu} % \bar is too narrow in math
\newcommand{\cost}{c}

\usepackage{nameref}
\usepackage{cleveref}

% use \S for all references to all kinds of sections, and \P to paragraphs
% (sadly, we cannot use the simpler \crefname{} macro because it would insert a space after the symbol)
\crefformat{part}{\S#2#1#3}
\crefformat{chapter}{\S#2#1#3}
\crefformat{section}{\S#2#1#3}
\crefformat{subsection}{\S#2#1#3}
\crefformat{subsubsection}{\S#2#1#3}
\crefformat{paragraph}{\P#2#1#3}
\crefformat{subparagraph}{\P#2#1#3}
\crefmultiformat{part}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
\crefmultiformat{chapter}{\S\S#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{section}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
\crefmultiformat{subsection}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
\crefmultiformat{subsubsection}{\S\S#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{paragraph}{\P\P#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{subparagraph}{\P\P#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefrangeformat{part}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{chapter}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{section}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{subsection}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{subsubsection}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{paragraph}{\mbox{\P\P#3#1#4--#5#2#6}}
\crefrangeformat{subparagraph}{\mbox{\P\P#3#1#4--#5#2#6}}
% for \label[appsec]{...}
\crefname{appsec}{appendix}{appendices}
\Crefname{appsec}{Appendix}{Appendices}
\crefname{enums,enumsi}{example}{examples}
\Crefname{enums,enumsi}{Example}{Examples}
\crefformat{enums}{(#2#1#3)}
\crefformat{enumsi}{(#2#1#3)}
\crefrangeformat{enums}{\mbox{(#3#1#4--#5#2#6)}}
\crefrangeformat{enumsi}{\mbox{(#3#1#4--#5#2#6)}}

\ifx\creflastconjunction\undefined%
\newcommand{\creflastconjunction}{, and\nobreakspace} % Oxford comma for lists
\else%
\renewcommand{\creflastconjunction}{, and\nobreakspace} % Oxford comma for lists
\fi%

\newcommand*{\Fullref}[1]{\hyperref[{#1}]{\Cref*{#1}: \nameref*{#1}}}
\newcommand*{\fullref}[1]{\hyperref[{#1}]{\cref*{#1}: \nameref{#1}}}
\newcommand{\fnref}[1]{\autoref{#1}} % don't use \cref{} due to bug in (now out-of-date) cleveref package w.r.t. footnotes


% Space savers
% From http://www.eng.cam.ac.uk/help/tpl/textprocessing/squeeze.html
%\addtolength{\dbltextfloatsep}{-.5cm} % space between last top float or first bottom float and the text.
%\addtolength{\intextsep}{-.5cm} % space left on top and bottom of an in-text float.
%\addtolength{\abovedisplayskip}{-.5cm} % space before maths
%\addtolength{\belowdisplayskip}{-.5cm} % space after maths
%\addtolength{\topsep}{-.5cm} %space between first item and preceding paragraph
%\setlength{\belowcaptionskip}{-.25cm}


% customize \paragraph spacing
\makeatletter
\renewcommand{\paragraph}{%
  \@startsection{paragraph}{4}%
  {\z@}{.2ex \@plus 1ex \@minus .2ex}{-1em}%
  {\normalfont\normalsize\bfseries}%
}
\makeatother



% Special macros
\newcommand{\tg}[1]{\texttt{#1}}	% supersense tag name
\newcommand{\lex}[1]{\textsmaller{\textsf{\textcolor{slate}{\textbf{#1}}}}}	% example lexical item 
\newcommand{\glosst}[2]{\ensuremath{\underset{\textrm{#2}}{\uline{\textrm{#1}}}}} % gloss text (a word or phrase) (second arg is the gloss)
\newcommand{\sys}[1]{\mbox{\textbf{#1}}}   % name of a system (one of our experimental conditions)
\newcommand{\dataset}[1]{\mbox{\textsc{#1}}}	% one of the datasets in our experiments
\newcommand{\datasplit}[1]{\mbox{\textbf{#1}}}	% portion one of the datasets in our experiments
\newcommand{\ilbl}[1]{\mbox{\textbf{\textsc{#1}}}} % internal label
\newcommand{\llbl}[1]{\mbox{\textsc{#1}}} % leaf label

\newcommand{\w}[1]{\textit{#1}}	% word
\newcommand{\tat}[0]{\textasciitilde}

\newcommand{\shortlong}[2]{#1} % short vs. long version of the paper
\newcommand{\confversion}[1]{#1}
\newcommand{\srsversion}[1]{}
%\newcommand{\finalversion}[1]{#1}
\newcommand{\finalversion}[1]{}
\newcommand{\costversion}[1]{}
\newcommand{\shortversion}[1]{#1}
\newcommand{\considercutting}[1]{#1}
\newcommand{\longversion}[1]{} % ...if only there were more space...
\newcommand{\subversion}[1]{#1} % for the submission version only
%\newcommand{\draftnotice}[1]{{\it\small #1}\\} % for the draft version only
\newcommand{\draftnotice}[1]{}
%\newcommand{\subversion}[1]{}

\hyphenation{WordNet}
\hyphenation{WordNets}
\hyphenation{VerbNet}
\hyphenation{FrameNet}
\hyphenation{PropBank}
\hyphenation{SemCor}
\hyphenation{SemLink}
\hyphenation{SEMAFOR}
\hyphenation{PennConverter}
\hyphenation{an-aly-sis}
\hyphenation{an-aly-ses}
\hyphenation{news-text}
\hyphenation{base-line}
\hyphenation{de-ve-lop-ed}
\hyphenation{comb-over}
\hyphenation{per-cept}
\hyphenation{per-cepts}
\hyphenation{post-edit-ing}

\newcommand{\vpred}[1]{\textbf{#1}} % verb predicate
\newcommand{\fname}[1]{\textsc{#1}} % frame name
\newcommand{\rname}[1]{\textsl{#1}} % role/frame element label
\newcommand{\luname}[1]{\textbf{#1}} % lexical unit name (predicate+POS)

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Leveraging Heterogeneous Data Sources for Relational Semantic Parsing}

\author{Meghana Kshirsagar
   \qquad
  Nathan Schneider
   \qquad
  Chris Dyer \\
  Language Technologies Institute \\
  Carnegie Mellon University, Pittsburgh PA\\
  {\tt \{mkshirsa,nschneid,cdyer\}@cs.cmu.edu} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
A number of semantic annotation efforts have produced a variety of annotated corpora, capturing various aspects of semantic knowledge in different formalisms. Due to to the cost of these annotation efforts and the relatively small amount of semantically annotated corpora, we argue it is advantageous to be able to leverage as much annotated data as possible. This work presents a preliminary exploration of the opportunities and challenges of learning semantic parsers from heterogeneous semantic annotation sources. 
We primarily focus on two semantic resources, FrameNet and PropBank, with the goal of improving frame-semantic parsing. 
Our analysis of the two data sources highlights the benefits that can be reaped by combining information across them.
\end{abstract}

\section{Introduction}
Multiple annotated resources capture relational semantic information, including FrameNet \citep{framenet}, PropBank \citep{propbank}, 
VerbNet \citep{vnet}, and the AMR Bank \citep{amr} for English. 
Each of these is the result of painstaking lexicography and annotation efforts by teams of linguists over several years. 
However, they are the result of largely independent annotation efforts which make different theoretical commitments. 
Here, we consider how we might best leverage such heterogenous resources to improve semantic parsing, 
so as to let semantic resources created at considerable cost not go to waste.

Supervised semantic parsers are usually trained with the representation and annotations from a single resource. 
For instance, SEMAFOR \citep{semafor,semafor-14} is a state-of-the-art frame-semantic parser
trained on the full text annotated corpus of FrameNet. 
PropBank has likewise served as the training resource for semantic role labeling (SRL) systems (e.g., \citealp{illinoisSRL}; see \citealp{palmer-10} for a review). 
FrameNet and PropBank seek to encode very similar aspects of meaning, including event predicates 
and their labeled arguments; yet the two projects have developed different representations, lexicons, 
and corpus annotation approaches due to different design considerations.
Broadly speaking, FrameNet contains richer forms of linguistic detail (e.g., predicates are semantically organized 
with respect to one other in FrameNet), but the PropBank representation is much more conducive to large-scale annotation, 
and PropBank therefore has much greater type and token coverage of verbs.
Moreover, the annotated corpora of FrameNet and PropBank capture different domains.
% Often, a single resource despite being very high quality does not contain comprehensive annotations 
% for every semantic entity and will not cover all semantic concepts. 
% For instance, PropBank annotations are very rich in verbs, but very sparse in nouns. 
% These resources also differ in the granularity of the annotated data---FrameNet frames 
% are semantically finer-grained as compared to the PropBank rolesets. 
As such, the two resources can be seen as partially overlapping but partially complementary.

\begin{figure*}\centering
\includegraphics[width=.8\textwidth]{tax_example_short.jpg}
\caption{Frame-semantic parse from SEMAFOR for a constructed example sentence.}
\label{fig:semaforOutput}
\end{figure*}


We hypothesize that systems could exploit this complementarity to gain robustness at semantic analysis. 
Encouragingly, some studies of \emph{syntactic} dependency parsing have demonstrated that divergent treebanks can be leveraged 
to improve parsing quality \citep{zhou:2013,johansson-13}.
A motivating example appears in \cref{sec:example}, followed by quantitative analysis of the resources in \cref{sec:quant}.

Initially, we had planned to use a third resource, SemLink, which provides mappings between the labels from FN and PB. However, based on our
analysis of this resource (presented in \cref{sec:semlink}), the mapping suffers from low coverage, and is ambiguous and noisy in some cases. 
Thus, our ability to map between the FN and PB representations via SemLink alone is limited.
Instead, the problem of combining the knowledge across these resources can be viewed as a 
transfer learning or joint learning problem (possibly using SemLink mappings as soft evidence). 
%If we treat the annotation schema of each resource as one label space, then we can formulate this 
%as a multitask learning problem across tasks with different label spaces. 
%The goal of a system should then be to learn a model over the different label spaces. 
Similar problems are faced in information extraction, where there are often multiple knowledge bases containing related information but conforming to different ontologies \citep[e.g.,][]{riedel-13}.  
We consider several possible modeling frameworks below in \cref{sec:modeling}. 



\section{Example}\label{sec:example}
Here we show the output parse from SEMAFOR (a frame-semantic parsing system based on FrameNet) for an example sentence. 
We will focus on the parse errors that are a result of the gaps in FN annotations.
\begin{quote}
\vpred{Taxing} wealthy individuals \vpred{appeals} to liberals; Brown \vpred{urges} \vpred{abolishing} regulations.
%\textit{The award \texttt{celebrates} businesses that \texttt{innovate} products which \texttt{appeal} to social good.}
\end{quote}
The four verbal predicates in this sentence are bolded. %\nss{bold for consistency with SEMAFOR output} 
\Cref{fig:semaforOutput} shows the frame-semantic parse in which SEMAFOR 
has predicted frame-evoking \emph{targets} (predicate tokens), 
\emph{frame labels} (displayed immediately below the targets), 
\emph{argument spans} (red lines) for each frame, and 
\emph{argument labels} (frame element names).
\begin{enumerate}
\item \vpred{urges} is mapped correctly to a frame, and its arguments to frame elements.
\item \vpred{individuals} receives the correct frame label, \fname{People}, but is missing an argument
(``wealthy'' should fill the \rname{Descriptor} frame element). In other wordings of this sentence,
``wealthy'' was correctly identified as an argument to \fname{People}, but incorrectly labeled
with the \rname{Ethnicity} frame element.
% the above were commented out, but that throws off the numbering from the discussion below and 
% the point that we should care about arg ID 
  \item \vpred{appeals} is annotated with the wrong frame label. The parse suggests that somebody is 
  making an appeal to liberals, whereas the correct analysis should use the \fname{Experiencer\_obj} frame 
  to represent that an idea (taxing wealthy individuals) provokes an emotional response on the part of liberals. 
  \item \vpred{abolishing} is not associated with any frame label, because it is absent from the FrameNet lexicon. 
  In principle, however, it should be recognized as evoking the \fname{Prohibiting} frame, which contains 
  synonymous verbs. 
 % SEMAFOR\finalversion{\nss{the demo version used for the figure?}} attempts to disambiguate such OOV lexical items 
 % to existing frames \citep{das-acl2011}.
  \item \vpred{taxing} is not identified as a target because it, too, is absent from the FrameNet lexicon. 
  However, there is no apparent home for it in any existing frame; a \fname{Taxation} frame would have to be defined 
  in the lexicon.\longversion{\footnote{As part of the definition process the frame would be linked to related concepts that already have frames, 
  such as \fname{Government\_institution}, \fname{Imposing\_obligation}, and \fname{Commerce\_collect}.}}
\end{enumerate}

All of these verbs are annotated in the PropBank corpus, which assigns a coarse sense number 
to each predicate and describes the predicate's core and non-core arguments. 
Even though PB and FN use different label spaces for predicates/frames and 
arguments/frame elements, we propose that PB could provide useful information for frame-semantic parsing 
with respect to (a)~detecting and labeling arguments (\#2 above),  
(b)~disambiguating targets that have multiple senses in both the PB and FN lexicons (\#3 above), and
(c)~labeling arguments where SEMAFOR is able to identify the correct frame for a target that is not in FN, 
provided that some other verbs in the same frame \emph{are} present in both PB and FN (\#4 above).


\section{Quantitative analysis}\label{sec:quant}

Here we quantify the extent to which the PropBank (PB) and FrameNet (FN) resources 
do or do not overlap. The analysis shows examples where gaps in one resource can be supplemented using information from the other. We obtain verb coverage measurements from version~1.5 of FrameNet 
and the PropBank WSJ (PB-WSJ) annotations in version~5.0 of OntoNotes \citep{ontonotes}. 

\subsection{Lexical coverage}\label{sec:coverage}

% The following list indicates the scenarios where a joint model based on both FrameNet and PropBank 
%  can improve frame-semantic parsing accuracies in a system like SEMAFOR. 
%  The examples and numbers presented below were obtained from the latest FrameNet release (1.5) 
%  and for PB we consider only the WSJ section of the annotations.
%\begin{itemize}

As noted above, type and token coverage is an advantage that PB has over FN, at least for verbs.

\paragraph{Type coverage.}
Overall, there are 1,260 verb types in the PB-WSJ data that are not present at all in the FN lexicon. 
These include \vpred{involve}, \vpred{lurk}, \vpred{nominate}, \vpred{ladle}, \vpred{entice}, and \vpred{bank}.
Hence a model that uses exclusively FrameNet data for training will not be able to correctly parse sentences 
containing these verb types.
There are also coverage gaps within existing frames: e.g., the \fname{Giving} frame has 19 known verbs as targets in FN, but  
many plausible members of this frame---\vpred{allot}, \vpred{assign}, \vpred{designate}, \vpred{allocate}, etc.---are not associated with any FN frame and do not appear in FN. Using the sentence annotations for these verbs from PB, we can obtain useful context information that will allow us to better predict suitable frames for them.

\paragraph{Corpus support for lexical types.} 984 of 4,894 verb senses (frame-disambiguated lexical units) in the FN lexicon 
never occur in the full-text annotations; many more occur only a few times.\finalversion{\nss{how many verb *types* occur at least once in PB?}}
For instance, the \fname{Manipulation} sense of \vpred{hold} is unattested in the full-text annotations,
whereas PB-WSJ has 177~instances (that had an extractable argument mapping) of the corresponding 
verb senses according to SemLink, \vpred{hold.01} and \vpred{hold.06}.
The frame \fname{Experiencer\_obj} has several predicates without any annotations, 
for example \vpred{appeal}, \vpred{harass}, \vpred{worry}, and \vpred{boggle}. 
The first three of these have PB-WSJ annotations. 
%\end{itemize}
%Ideally, a joint learning system should also be able to suggest new targets for a frame based on the lexical similarity with the frame's existing targets.

\subsection{SemLink}\label{sec:semlink}
The SemLink database \citep{semlink} specifies mappings between PropBank, FrameNet, and VerbNet. 
We analyzed the mappings between PropBank and FrameNet in order to determine their reliability in building a joint model.

SemLink contains two types of mappings. The \emph{sense-level mappings} give correspondences between the concepts
from each resource---i.e., between `frames' from FN, `rolesets' from PB, and `roles' from VN. 
Since they map different interpretations and granularities 
of concepts, the sense-level mappings may be one-to-one, one-to-many, or many-to-many.
PropBank and FrameNet are mapped indirectly via VerbNet. %, introducing additional coverage gaps and ambiguities.
Second, SemLink provides some \emph{token-level parallel annotations} 
for the 3~representations in a subset of the PB-WSJ text: hereafter SL-WSJ.

We focus on using token-level SemLink version~1.2.2c annotations as a (disambiguated) mapping from PB to FN tokens.
%In this work, we will focus on SEMAFOR as the target system whose performance we want to improve. We are hence interested in using the SemLink mappings to get FN compatible data. 
%The type (a) mappings can be directly used to augment the FN annotations; in the next section we present an analysis on the quantity and usability of the available mappings. Using the type (b) annotations first requires disambiguating the frame-to-roleset labels and then aligning the predicate arguments of the roleset with those expected in the frame. Note that the two argument sets might be of different cardinality.
Some statistics appear in \cref{tab:wsjstats,tab:annoUnit}. The first table summarizes the SemLink mappings in the SL-WSJ data. 
Of 74,977 SL-WSJ verbs, a majority cannot be mapped to FN labels for various reasons. 
Around 31\% of the predicates have the frame label \texttt{IN} (``indefinite'') where the mapping from VerbNet to FrameNet is ambiguous. 
About 20\% of the instances are labeled \texttt{NF} (``no frame''), indicating a coverage gap in FrameNet.\finalversion{\nss{did SL annotators attempt to find frames for verbs not in FN?}}
21\% of verbs have frame labels but no frame element annotations. 
Most of these are predicates with modifier arguments. 
Other arguments pointed to null anaphora that could not be resolved to overt arguments.
This leaves 15,323 mappable instances with at least one overt argument, or 20\% of SL-WSJ verbs. This is a very small
subset of the entire PB annotated data.

The second table compares the extracted SL-WSJ annotations with the full-text FN annotations used to train SEMAFOR. 
%The number of annotations at the sentence-level and frame-level are shown. 
It seems that the PB sentences contain a higher rate of annotated verbs per sentence than FN, 
likely due in part to gaps in coverage for FN.\footnote{Other possible explanations include domain differences and different conventions for light verbs.}
There is some dropoff in annotation density when mapping from PB to FN due to coverage gaps in SemLink.

\begin{table}\centering\small
\begin{tabular}{@{}>{\raggedright}p{13em}@{~~}r@{~~}r@{}}
\bf FN frame annotation & \bf PB verb tokens & \bf \% of all \\ \hline
Frame label = \texttt{NF} & 14,624 & 20\%\\ 
Frame label = \texttt{IN} & 22,982 & 31\% \\ 
Frame with no arguments & 15,533 & 21\% \\ 
Frame with at least 1~mappable argument  & 15,323 & 20\% \\ 
Instances not mapped due to other issues & 6,516 & 9\% \\ \hline 
Total  & 74,977 & 100\% \\
\end{tabular}
\caption{Statistics of PB-WSJ data from SemLink}
\label{tab:wsjstats}
\end{table}

\begin{table}\centering\small
\begin{tabular}{p{17em}r} 
\bf Annotation unit & \bf Count \\\hline 
Sentences in FN 1.5 & 5,946\finalversion{\nss{double-check. p. 18 of CL article suggests 5676. unclear whether that includes dev set}} \\ 
%Frame annotations in FN 1.5 (all POSes) & 23,940 \\ % new framenet (website): 27409
%Annotation density & 8.6 \\ \hline 
FN frame annotations for verb predicates & 6,993 \\
Verb annotation density & 1.2 \\ \hline
%PB-WSJ sentences &  38,594 \\ % this is PB 1.0 
%PB-WSJ verb tokens & 74,977 \\ % this is PB 1.0
PB-WSJ sentences &  35,426 \\ 
PB-WSJ verb tokens & 96,517 \\ % 103630 all POSes
Verb annotation density &  2.7 \\\hline
SL-WSJ sentences with at least one mappable FN frame and argument & 12,382\\ 
Mapped frame annotations & 15,323\\ 
Verb annotation density & 1.2 \\
\end{tabular}
\caption{Comparison of annotation density\finalversion{\nss{consider showing: predicates/sentence, predicates/word, predicates/verb, args/predicate}}}
\label{tab:annoUnit}
\end{table}

The mappings we obtained increase the number of annotations for around 170 frames. 
Figure \ref{fig:framesBarchart} shows a stacked bar chart plotting the annotations for every frame, 
sorted in decreasing order of additional frames obtained. The highest is around 1,500 new annotations for the \fname{Statement} frame.
\begin{figure*}\centering
\includegraphics[scale=0.5]{framesBarchart.pdf}
\caption{The number of new annotations per frame obtained upon extracting the WSJ mapping from SemLink. Red bars indicate the contribution from the new annotations. Only frames for which new annotations were found are shown.}
\label{fig:framesBarchart}
\end{figure*}

\paragraph{Noise in SemLink.}
Some of the FN information in SemLink is out of date due to subsequent changes in FrameNet.
For example, the frame \fname{Statement} no longer contains the lexical unit \luname{complain.v}, 
which has been moved to a new frame (\fname{Complaining}). 
There are around 3,000 such instances with obsolete annotations. 
Some of them may be updated automatically using the Reframing\_mapping pointers in FrameNet, 
but some may have to be reannotated.
There are some erroneous FN annotations as well: e.g., all 14~instances of \vpred{liquidate} are 
labeled \fname{Killing}, despite being used in the financial sense; and in 17~cases \vpred{direct} 
is erroneously marked as \fname{Behind\_the\_scenes} (i.e., film direction). 
Therefore, we suspect that heavy reliance on the SemLink annotations in a model will be a source of precision errors.

\finalversion{Additionally, there are some mistakes in some of the annotations due to the existence of multiple frame matches for a particular predicate. 
For example, in the sentence ``McMoRan Energy Partners will be \vpred{liquidated}'', 
the frame for \vpred{liquidate} is \fname{Killing}---all 14 occurrences of liquidate have this error. 
The sentence ``Speaker Jim Wright\ldots attempting to \vpred{direct} the president'' has the frame annotation \fname{Behind\_the\_scenes}, 
which refers to film direction. There are 17 instances with this frame erroneously marked. These kind of errors are hard to detect. 
The SemLink mappings can thus not be used as gold-standard annotations to train models. One possibility is to use this data as low-confidence training data.}

\section{Modeling}\label{sec:modeling}
This problem of learning from multiple resources can be formulated in different ways. We present here a spectrum of possible approaches. 
%each relying either on a different methodology or manipulating the available data differently. 
For simplicity, let us assume there are two resources, $D_1$ and $D_2$. Let $X_1, X_2 \in \mathcal{X}$ represent the set of data instances (i.e.~sentences) from the two sources and $Y_1 \in \mathcal{Y}$, $Y_2 \in \mathcal{Y}'$ be the labels.
\begin{itemize}
\item \textbf{Deterministically unifying the schemas for the training data.} With a mapping $\phi : \mathcal{Y}' \rightarrow \mathcal{Y}$ between the label spaces from the two resources, the annotations from one resource can be transformed into the labeling schema used by the other, in order to train a single model 
on $\langle X_1, Y_1\rangle \cup \langle X_2, \phi(Y_2)\rangle$. The (limited and ambiguous) SemLink mappings discussed above are one possible choice of $\phi$; 
a latent mapping could also be learned, treating the noisy correspondences in SemLink as evidence.
\item \textbf{Learning and decoding as a pipeline.} Training a source model on $D_1$ and applying it on $X_2$ to extract features for training a target model on $D_2$. 
\Citet{johansson-13} calls this \emph{guided parsing}. 
\item \textbf{Bootstrapping} with approaches such as co-training \citep{blum-98,clark03}, where two separate models are iteratively improved by 
providing high-confidence pseudo-annotated training examples for each other. A similar approach is taken in \citet{zhou:2013} for dependency parsing.
% or , where we iteratively improve one model using the output from the other. This is the same as the pipeline approach described above, performed for several iterations. At iteration $t$, we obtain $\theta_1^t$ and apply it on $X_2$. The output labels are used as features along with $X_2$ to train the model $\theta_2^t$. In the next iteration $(t+1)$, the output labels of $\theta_2^t$ applied on $X_1$ can be used as features to train $\theta_1^{t+1}$.
\item \textbf{Optimizing a multi-task objective} with a loss function over the two data sources $D_1$ and $D_2$. 
The feature spaces from the two tasks can be combined \citep{daume-07,johansson-13} or coupled together using a feature transformation \citep{mtfl,scl}.
\end{itemize}

%\section{Evaluation}
%As indicated in the introduction, a model encompassing multiple resources can fill in various types of gaps in the FN annotations. It is infeasible to measure improvements for predicates which are not already frame targets in FN. For existing frame-predicates with no argument annotations, the SemLink mappings can be used as test data.

\section{Conclusion}

Based on our analysis, we can conclude that learning models from heterogeneous data sources 
is a promising direction for improving the performance of semantic parsing systems.
With several learning frameworks at our disposal, we hope to develop an approach 
that compensates for gaps in both lexical and corpus coverage to improve performance, particularly for the FrameNet task.

\finalversion{\section*{Acknowledgments}

\nss{TODO}}

\bibliographystyle{aclnat}
\setlength{\bibsep}{0pt}
{\fontsize{10}{12.25}\selectfont
\bibliography{refs}}

\end{document}
