\PassOptionsToPackage{usenames}{color}
\documentclass[11pt]{article}
\usepackage{comment}
\usepackage{relsize} % relative font sizes (e.g. \smaller). must precede ACL style
\usepackage{acl2014}

\usepackage[linkcolor=blue]{hyperref}

\usepackage[round]{natbib}
\newcommand{\citeposs}[1]{\citeauthor{#1}'s (\citeyear{#1})}

%\usepackage{times}
%\usepackage{latexsym}


\usepackage[boxed]{algorithm2e}
\renewcommand\AlCapFnt{\small}
\usepackage[small,bf,skip=5pt]{caption}
\usepackage{sidecap} % side captions
\usepackage{rotating}	% sideways

% Italicize subparagraph headings
\usepackage{titlesec}
\titleformat*{\subparagraph}{\itshape}
\titlespacing{\subparagraph}{%
  1em}{%              left margin
  0pt}{% space before (vertical)
  1em}%               space after (horizontal)

% Numbered Examples and lists
\usepackage{lingmacros}

\usepackage{enumitem} % customizable lists
\setitemize{noitemsep,topsep=0em,leftmargin=*}
\setenumerate{noitemsep,leftmargin=0em,itemindent=13pt,topsep=0em}


\usepackage{textcomp}
% \usepackage{arabtex} % must go after xparse, if xparse is used!
%\usepackage{utf8}
% \setcode{utf8} % use UTF-8 Arabic
% \newcommand{\Ar}[1]{\RL{\novocalize #1}} % Arabic text

\usepackage[procnames]{listings}

\usepackage{amssymb}	%amsfonts,eucal,amsbsy,amsthm,amsopn
\usepackage{amsmath}

\usepackage{mathptmx}	% txfonts
\usepackage[scaled]{beramono}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}

\usepackage{MnSymbol}	% must be after mathptmx

\usepackage{latexsym}





% Tables
\usepackage{array}
\usepackage{multirow}
\usepackage{booktabs} % pretty tables
\usepackage{multicol}
\usepackage{footnote}


\usepackage{url}
\usepackage[usenames]{color}
\usepackage{xcolor}

% colored frame box
\newcommand{\cfbox}[2]{%
    \colorlet{currentcolor}{.}%
    {\color{#1}%
    \fbox{\color{currentcolor}#2}}%
}

\usepackage[normalem]{ulem} % \uline
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{subcaption}
%\usepackage{tikz-dependency}
\usepackage{tikz}
%\usepackage{tree-dvips}
\usetikzlibrary{arrows,positioning,calc} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{verbatim}
\usepackage{ulem}


% Author comments
\usepackage{color}
\newcommand\bmmax{0} % magic to avoid 'too many math alphabets' error
\usepackage{bm}
\definecolor{orange}{rgb}{1,0.5,0}
\definecolor{mdgreen}{rgb}{0,0.6,0}
\definecolor{mdblue}{rgb}{0,0,0.7}
\definecolor{dkblue}{rgb}{0,0,0.5}
\definecolor{dkgray}{rgb}{0.3,0.3,0.3}
\definecolor{slate}{rgb}{0.25,0.25,0.4}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{ltgray}{rgb}{0.7,0.7,0.7}
\definecolor{purple}{rgb}{0.7,0,1.0}
\definecolor{lavender}{rgb}{0.65,0.55,1.0}

% Settings for algorithm listings
\makeatletter
\lst@AddToHook{EveryPar}{%
  \label{lst:\thelstnumber}% make a label for each line number except the first (assumes only one listing in the document)
}
\makeatother
\lstset{
% basicstyle=\rmshape,
  numbers=left,
  numberstyle=\tt\color{gray},
  firstnumber=2,
  stepnumber=5,
  xleftmargin=3em,
  language=Python,
  upquote=true,
  showstringspaces=false,
  formfeed=\newpage,
  tabsize=1,
  stringstyle=\color{mdgreen},
  commentstyle=\itshape\color{lavender},
  basicstyle=\small\smaller\ttfamily,
  morekeywords={lambda,with,as,assert},
  keywordstyle=\bfseries\color{magenta},
  procnamekeys={def},
  procnamestyle=\bfseries\color{orange},
  aboveskip=0.5cm,
  belowskip=0.5cm
}
\renewcommand{\lstlistingname}{Algorithm}


\newcommand{\ensuretext}[1]{#1}
\newcommand{\cjdmarker}{\ensuretext{\textcolor{green}{\ensuremath{^{\textsc{CJ}}_{\textsc{D}}}}}}
\newcommand{\nssmarker}{\ensuretext{\textcolor{magenta}{\ensuremath{^{\textsc{NS}}_{\textsc{S}}}}}}
\newcommand{\abmarker}{\ensuretext{\textcolor{red}{\ensuremath{^{\textsc{A}}_{\textsc{B}}}}}}
\newcommand{\ytmarker}{\ensuretext{\textcolor{blue}{\ensuremath{^{\textsc{Y}}_{\textsc{T}}}}}}
\newcommand{\ftamarker}{\ensuretext{\textcolor{orange}{\ensuremath{^{\textsc{FT}}_{\textsc{A}}}}}}
\newcommand{\lslmarker}{\ensuretext{\textcolor{purple}{\ensuremath{^{\textsc{LS}}_{\textsc{L}}}}}}
\newcommand{\arkcomment}[3]{\ensuretext{\textcolor{#3}{[#1 #2]}}}
%\newcommand{\arkcomment}[3]{}
\newcommand{\cjd}[1]{\arkcomment{\cjdmarker}{#1}{green}}
\newcommand{\nss}[1]{\arkcomment{\nssmarker}{#1}{magenta}}
\newcommand{\ab}[1]{\arkcomment{\abmarker}{#1}{red}}
\newcommand{\yt}[1]{\arkcomment{\ytmarker}{#1}{blue}}
\newcommand{\fta}[1]{\arkcomment{\ftamarker}{#1}{orange}}
\newcommand{\lsl}[1]{\arkcomment{\lslmarker}{#1}{purple}}
\newcommand{\params}{\mathbf{\theta}}
\newcommand{\wts}{\mathbf{w}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu} % \bar is too narrow in math
\newcommand{\cost}{c}

\usepackage{nameref}
\usepackage{cleveref}

% use \S for all references to all kinds of sections, and \P to paragraphs
% (sadly, we cannot use the simpler \crefname{} macro because it would insert a space after the symbol)
\crefformat{part}{\S#2#1#3}
\crefformat{chapter}{\S#2#1#3}
\crefformat{section}{\S#2#1#3}
\crefformat{subsection}{\S#2#1#3}
\crefformat{subsubsection}{\S#2#1#3}
\crefformat{paragraph}{\P#2#1#3}
\crefformat{subparagraph}{\P#2#1#3}
\crefmultiformat{part}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
\crefmultiformat{chapter}{\S\S#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{section}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
\crefmultiformat{subsection}{\S#2#1#3}{ and~\S#2#1#3}{, \S#2#1#3}{, and~\S#2#1#3}
\crefmultiformat{subsubsection}{\S\S#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{paragraph}{\P\P#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefmultiformat{subparagraph}{\P\P#2#1#3}{ and~#2#1#3}{, #2#1#3}{, and~#2#1#3}
\crefrangeformat{part}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{chapter}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{section}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{subsection}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{subsubsection}{\mbox{\S\S#3#1#4--#5#2#6}}
\crefrangeformat{paragraph}{\mbox{\P\P#3#1#4--#5#2#6}}
\crefrangeformat{subparagraph}{\mbox{\P\P#3#1#4--#5#2#6}}
% for \label[appsec]{...}
\crefname{appsec}{appendix}{appendices}
\Crefname{appsec}{Appendix}{Appendices}
\crefname{enums,enumsi}{example}{examples}
\Crefname{enums,enumsi}{Example}{Examples}
\crefformat{enums}{(#2#1#3)}
\crefformat{enumsi}{(#2#1#3)}
\crefrangeformat{enums}{\mbox{(#3#1#4--#5#2#6)}}
\crefrangeformat{enumsi}{\mbox{(#3#1#4--#5#2#6)}}

\ifx\creflastconjunction\undefined%
\newcommand{\creflastconjunction}{, and\nobreakspace} % Oxford comma for lists
\else%
\renewcommand{\creflastconjunction}{, and\nobreakspace} % Oxford comma for lists
\fi%

\newcommand*{\Fullref}[1]{\hyperref[{#1}]{\Cref*{#1}: \nameref*{#1}}}
\newcommand*{\fullref}[1]{\hyperref[{#1}]{\cref*{#1}: \nameref{#1}}}
\newcommand{\fnref}[1]{\autoref{#1}} % don't use \cref{} due to bug in (now out-of-date) cleveref package w.r.t. footnotes


% Space savers
% From http://www.eng.cam.ac.uk/help/tpl/textprocessing/squeeze.html
%\addtolength{\dbltextfloatsep}{-.5cm} % space between last top float or first bottom float and the text.
%\addtolength{\intextsep}{-.5cm} % space left on top and bottom of an in-text float.
%\addtolength{\abovedisplayskip}{-.5cm} % space before maths
%\addtolength{\belowdisplayskip}{-.5cm} % space after maths
%\addtolength{\topsep}{-.5cm} %space between first item and preceding paragraph
%\setlength{\belowcaptionskip}{-.25cm}


% customize \paragraph spacing
\makeatletter
\renewcommand{\paragraph}{%
  \@startsection{paragraph}{4}%
  {\z@}{.2ex \@plus 1ex \@minus .2ex}{-1em}%
  {\normalfont\normalsize\bfseries}%
}
\makeatother



% Special macros
\newcommand{\tg}[1]{\texttt{#1}}	% supersense tag name
\newcommand{\lex}[1]{\textsmaller{\textsf{\textcolor{slate}{\textbf{#1}}}}}	% example lexical item 
\newcommand{\glosst}[2]{\ensuremath{\underset{\textrm{#2}}{\uline{\textrm{#1}}}}} % gloss text (a word or phrase) (second arg is the gloss)
\newcommand{\sys}[1]{\mbox{\textbf{#1}}}   % name of a system (one of our experimental conditions)
\newcommand{\dataset}[1]{\mbox{\textsc{#1}}}	% one of the datasets in our experiments
\newcommand{\datasplit}[1]{\mbox{\textbf{#1}}}	% portion one of the datasets in our experiments
\newcommand{\ilbl}[1]{\mbox{\textbf{\textsc{#1}}}} % internal label
\newcommand{\llbl}[1]{\mbox{\textsc{#1}}} % leaf label

\newcommand{\w}[1]{\textit{#1}}	% word
\newcommand{\tat}[0]{\textasciitilde}

\newcommand{\shortlong}[2]{#1} % short vs. long version of the paper
\newcommand{\confversion}[1]{#1}
\newcommand{\srsversion}[1]{}
%\newcommand{\finalversion}[1]{#1}
\newcommand{\finalversion}[1]{}
\newcommand{\costversion}[1]{}
\newcommand{\shortversion}[1]{#1}
\newcommand{\considercutting}[1]{#1}
\newcommand{\longversion}[1]{} % ...if only there were more space...
\newcommand{\subversion}[1]{#1} % for the submission version only
%\newcommand{\draftnotice}[1]{{\it\small #1}\\} % for the draft version only
\newcommand{\draftnotice}[1]{}
%\newcommand{\subversion}[1]{}

\hyphenation{WordNet}
\hyphenation{WordNets}
\hyphenation{VerbNet}
\hyphenation{FrameNet}
\hyphenation{PropBank}
\hyphenation{SemCor}
\hyphenation{SEMAFOR}
\hyphenation{PennConverter}
\hyphenation{an-aly-sis}
\hyphenation{an-aly-ses}
\hyphenation{news-text}
\hyphenation{base-line}
\hyphenation{de-ve-lop-ed}
\hyphenation{comb-over}
\hyphenation{per-cept}
\hyphenation{per-cepts}
\hyphenation{post-edit-ing}

\newcommand{\vpred}[1]{\textbf{#1}}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Heterogeneous Supervisory Data Sources for Semantic Parsing}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
A number of semantic annotation efforts have produced a variety of annotated corpora, capturing various aspects of semantic knowledge in different formalisms. Due to to the cost of these annotation efforts and the relatively small amount of semantically annotated corpora, we argue it is advantageous to be able to leverage as much annotated data as possible. This work presents a preliminary exploration of the opportunities and challenges of learning semantic parsers from heterogeneous semantic annotation sources. 
\end{abstract}

\section{Introduction}
Multiple annotated resources capture semantic information, for example FrameNet \citep{framenet}, PropBank \citep{propbank}, VerbNet \citep{vnet}. Each of them the result of pain-staking annotation efforts by teams of linguistics over several years. However, they are the result of largely independent annotation efforts which different make theoretical commitments. In this essay, we consider how we might best leverage such heterogenous resources to improve semantic parsing, so as to let semantic resources created at considerable cost not go to waste.

Systems for semantic parsing tasks have mostly focused on using only a single resource. For instance, SEMAFOR \citep{semafor} is a frame-semantic parsing system based on supervised learning that uses FrameNet data to train a model. The Illinois-semantic role labeling system \citep{illinoisSRL} uses PropBank annotations. Often, a single resource despite being very high quality does not contain comprehensive annotations for every semantic entity and will not cover all semantic concepts. For instance, PropBank annotations are very rich in verbs, but very sparse in nouns. These resources also differ in the granularity of the annotated data - FrameNet frames are semantically finer-grained as compared to the PropBank rolesets. However, together these complementary sources provide a very good and diverse coverage of the semantic space - a fact that should be exploited by systems in order to achieve a better performance on semantic analysis. Preliminary work in dependency parsing has demonstrated that divergent treebanks can be leveraged to improve parsing quality \citep{zhou:2013}.

From a supervised learning perspective, the problem of combining the knowledge across these resources can be viewed as a joint learning problem. If we treat the annotation schema of each resource as one label-space, then we can formulate this as a multitask learning problem across tasks with different label spaces. The goal of a system should then be to learn a model over the different label spaces. This problem also manifests in the information extraction community, where there are multiple knowledge-bases confirming to different ontologies.  

To see an example illustrating the need for models that integrate resources, consider the following sentence:
\begin{quote}
\vpred{Taxing} wealthy individuals \vpred{appeals} to liberals; Brown \vpred{urges} \vpred{abolishing} regulations.
%\textit{The award \texttt{celebrates} businesses that \texttt{innovate} products which \texttt{appeal} to social good.}
\end{quote}
The four verbal predicates in this sentence are in bold. 
\Cref{fig:semaforOutput} shows the frame-semantic parse predicted by SEMAFOR. 
We observe:
\begin{enumerate}
  \item \vpred{urges} is mapped correctly to a frame, and its arguments to frame elements. 
  \item \vpred{appeals} is correctly identified as a target, but the wrong frame label is chosen 
  (a sense disambiguation error). The parse suggests that somebody is making an appeal to liberals, 
  whereas the correct analysis would use the Experiencer\_obj frame to represent that an idea
  (taxing wealthy individuals) provokes an emotional response on the part of liberals. 
  \nss{mention that SEMAFOR can be run as a hard pipeline or can use beam search b/w frame and arg ID?}
  \item \vpred{abolishing} is not identified as a target because it is absent from the FrameNet lexicon. 
  In principle, however, it should be recognized as evoking the Prohibiting frame, which contains 
  nearly synonymous verbs. SEMAFOR\nss{the online version?} attempts to disambiguate such OOV lexical items 
  to existing frames \cite{das-acl2011}.
  \item \vpred{taxing} is not identified as a target because it, too, is absent from the FrameNet lexicon. 
  However, there is no apparent home for it in any existing frame; a Taxation frame would have to be defined 
  in the lexicon (and linked to related concepts that already have frames, such as Government\_institution, 
  Imposing\_obligation, and Commerce\_collect).
\end{enumerate}

All of these verbs are annotated in the PropBank corpus.\nss{elaborate}

\begin{figure*}
\includegraphics[width=\textwidth]{tax_example_short.jpg}
\caption{Frame semantic parse from SEMAFOR for a constructed example sentence.}
\label{fig:semaforOutput}
\end{figure*}


The following list indicates the scenarios where a joint model based on both FrameNet and PropBank data can improve frame-semantic parsing accuracies in a system like SEMAFOR. The examples and numbers presented below were obtained from the latest FrameNet release (1.5) and for PB we consider only the WSJ section of the annotations.
\begin{itemize}
\item Frames with none or very few annotations in FN. The frame \textit{Manipulation} does not have any annotations for the verb \texttt{hold}, whereas PB-WSJ has 117 instances (that had an extractable argument mapping) for this frame-target combination. The frame \textit{Experiencer\_obj} has several predicates without any annotations, for example \texttt{appeal}, \texttt{harass}, \texttt{worry}, \texttt{boggle}. The first three of these have annotations in the PB-WSJ data. There are 984 such predicates in FN that have been assigned to frames, but with no sentence annotations available.
\item Frames with few known targets or targets with no associated frames. For example, the frame \textit{Giving} has 19 known verbs as targets in FN. Synonyms of \textit{giving} such as \texttt{allot}, \texttt{assign}, \texttt{designate}, \texttt{allocate} are not present in FN. Overall, there are 475 verbs in the PB WSJ data that are not targets for any frame in FrameNet. Some examples are: \texttt{involve}, \texttt{lurk}, \texttt{nominate}, \texttt{ladle}, \texttt{entice}, \texttt{bank}.
\end{itemize}
Ideally, a joint learning system should also be able to suggest new targets for a frame based on the lexical similarity with the frame's existing targets.

Linguistic resources to map the different annotations have been built, but to a very limited extent. SemLink \cite{semlink} is one such database that maps and unifies different lexical resources of semantic information: such as PropBank (PB), FrameNet (FN), VerbNet (VN). The mappings that SemLink provides are available at two different levels: (a) sentence level parallel annotations - the WSJ section of the PB data has been annotated with the appropriate frames and frame-elements as well as VerbNet classes wherever possible, thereby giving detailed PB, VN and FN annotations for each sentence (b) concept/roleset level mappings---these are coarse-level mappings defined between a PB roleset and a VN roleset or a FN frame and a VN roleset. These mappings can be one-to-one, one-to-many or many-to-many depending on the semantic generality of the involved rolesets. To go from a PB roleset to a FN frame, one has to go via the VN roleset first.

In this work, we will focus on SEMAFOR as the target system whose performance we want to improve. We are hence interested in using the SemLink mappings to get FN compatible data. The type (a) mappings can be directly used to augment the FN annotations; in the next section we present an analysis on the quantity and usability of the available mappings. Using the type (b) annotations first requires disambiguating the frame-to-roleset labels and then aligning the predicate arguments of the roleset with those expected in the frame. Note that the two argument sets might be of different cardinality.

\section{Data analysis}
We present some statistics of the data resources in Tables \ref{tab:wsjstats} and \ref{tab:annoUnit}. The first table summarizes the SemLink mappings for the PB WSJ data. The total number of annotated PB instances are 74977, a majority of which do not have the corresponding FN labels due to various difficulties in mapping them. Around 31\% of the predicates have the frame label \textit{IN} meaning ``indefinite''. For these cases the mapping from VN to FN does not clearly indicate which Frame the instance should be. About 20\% of the instances are labeled \textit{NF} or ``no frame'' as FrameNet currently does not have an appropriate frame class for the semantic concept represented by the PB predicate. Of the remaining, 21\% are incomplete as they only have frame labels and no argument annotations. Most of these are predicates with modifier arguments. This leaves about 29\% usable mappings, 9\% of which had argument pointer and other issues. Eventually, we were able to extract about 15323 instances.

The second table compares the extracted PB-WSJ annotations with the original FN annotations (version 1.4) used to train the SEMAFOR model. The number of annotations at the sentence-level and frame-level are shown. It is evident that FN has a much higher annotation density - around 9 frames per sentence, as compared to around 1.2 for the PB derived data .


\begin{table}
\caption{Statistics of PB-WSJ data from SemLink}
\label{tab:wsjstats}
\begin{tabular}{|l|c|c|} \hline
FN frame annotation & Verb tokens & \% of all \\
& from PB & \\ \hline \hline
Frame label = \textit{NF} & 14624 & 20\%\\ \hline
Frame label = \textit{IN} & 22982 & 31\% \\ \hline
Frame with no & 15533 & 21\% \\ 
arguments  & & \\\hline
Frame with at least & 15323 & 20\% \\ 
1 argument mappable & & \\ \hline
Instances not mapped & 6516 & 9\% \\ 
due to other issues & & \\ \hline \hline
Total PB predicate & 74977 & 100\% \\
tokens (instances) & & \\ \hline
\end{tabular}
\end{table}

\begin{table}
\caption{Comparison of annotation density}
\begin{tabular}{|l|c|} \hline
\label{tab:annoUnit}
Annotation unit & Count \\\hline \hline
Sentences in FN 1.4 & 2780 \\ 
Annotation sets in FN 1.4 & 23940 \\ % new framenet: 27409
(i.e frame instances) & \\ 
Annotation density & 8.6 \\ \hline \hline
Sentences in PB WSJ data &  38594 \\ 
Verb tokens in PB WSJ & 74977 \\ 
Annotation density &  1.9 \\\hline \hline
Sentences extracted & 12382\\ 
from SemLink PB-WSJ & \\ 
Annotation sets extracted & 15323\\ 
Annotation density & 1.2 \\ \hline 
\end{tabular}
\end{table}



The mappings we obtained increase the number of annotations for around 170 frames. Figure \ref{fig:framesBarchart} shows a stacked bar-chart plotting the annotations for every frame, sorted in decreasing order of additional frames obtained. The highest is around 1500 new annotations for the frame \textit{Statement}.
\begin{figure*}
\caption{The number of new annotations per frame obtained upon extracting the WSJ mapping from SemLink. The bars are coloured (red) to indicate the contribution from the new annotations. Only frames for which new annotations were found are shown.}
\includegraphics[scale=0.5]{framesBarchart.pdf}
\label{fig:framesBarchart}
\end{figure*}

\subsection{Noisy SemLink annotations}
Since SemLink was built, FrameNet has updated frame definitions, some new frames were introduced and old ones deleted or divided into finer categories, irrelevant predicates moved to proper frames etc. Hence some of the SemLink annotations are obsolete and sub-optimal. For example, the frame \textit{Statement} does not contain the target \texttt{complain.v} anymore as a new frame \textit{Complain} was introduced. There are around 3000 such instances with obsolete annotations. These can be detected easily and they are also not semantically absurd, but correcting these will require re-annotation on a case-by-case basis.

Additionally, there are some mistakes in some of the annotations due to the existence of multiple frame matches for a particular predicate. For example, in the sentence \textit{McMoRan Energy Partners will be \texttt{liquidated}}, the frame for \texttt{liquidate} is \textit{Killing} - all 14 occurrences of liquidate have this error. The sentence \textit{Speaker Jim Wright.. attempting to \texttt{direct} the president} has the frame annotation \textit{Behind\_the\_scenes} which refers to film ``direction''. There are 17 instances with this frame erroneously marked. These kind of errors are hard to detect. The SemLink mappings can thus not be used as gold-standard annotations to train models. One possibility is to use this data as low-confidence training data.

\section{Models for joint learning}
This problem of learning from multiple resources can be formulated in different ways. We present here a spectrum of possible approaches, each relying either on a different methodology or manipulating the available data differently. For simplicity, let us assume there are two resources $D_1, D_2$. Let $X_1, X_2 \in \mathcal{X}$ represent the set of data instances (i.e sentences) from the two sources and $Y_1 \in \mathcal{Y}$, $Y_2 \in \mathcal{Y}'$ be the labels.
\begin{itemize}
\item Learning a deterministic mapping $\phi : \mathcal{Y}' \rightarrow \mathcal{Y}$ between the label spaces from the two resources. Such a mapping can be used to transform the annotations from one resource into the labeling schema used by the other. We could then train a model using $\{X_1, Y_1\} \cup \{X_2, \phi(Y_2)\}$. The SemLink mappings we discussed earlier give us such a function $\phi$ that gives a limited and ambiguous mapping.
\item A two-stage pipeline that trains a model on $D_1$ say $\theta_1$ and apply it on $X_2$. The output labels can be used as features along with $X_2$ to train the model $\theta_2$ on $D_2$. 
\item A bootstrapping approach that uses co-training similar to \cite{clark03}, where we iteratively improve one model using the output from the other. This is the same as the pipeline approach described above, performed for several iterations. At iteration $t$, we obtain $\theta_1^t$ and apply it on $X_2$. The output labels are used as features along with $X_2$ to train the model $\theta_2^t$. In the next iteration $(t+1)$, the output labels of $\theta_2^t$ applied on $X_1$ can be used as features to train $\theta_1^{t+1}$.
\item A multi-task learning based objective function that maximizes the log likelihood over the two data sources $D_1$ and $D_2$. The parameters from the two tasks can be coupled together using a feature transformation \cite{mtfl}, or based on feature correspondences learned using unlabeled data \cite{scl}.
\end{itemize}

\section{Evaluation}
As indicated in the introduction, a model encompassing multiple resources can fill in various types of gaps in the FN annotations. It is infeasible to measure improvements for predicates which are not already frame targets in FN. For existing frame-predicates with no argument annotations, the SemLink mappings can be used as test data.

\section*{Acknowledgments}

% include your own bib file like this:
\bibliographystyle{aclnat}
\bibliography{refs}

\end{document}
